Log Parser
==

Parse the **access.log** file, fill a database and block IPs.
This implementation follow [this requirements](Java_MySQL_Test_Instructions.md).

*All the script were test **only** into a ubuntu 16.04 machine.*

# Table of Contents

   * [Dependencies](#dependencies)
   * [Architecture choices](#architecture-choices)
   * [Build](#build)
      * [Executable](#executable)
      * [Infrastructure](#infrastructure)
   * [Usage](#usage)
   * [Database](#database)
      * [Schema](#schema)
         * [Indexes](#indexes)
      * [Quering](#quering)

# Dependencies

* Java 8
* Maven
* Docker
* Docker-compose

# Architecture choices

I choose use in this implementation only Hibernate.

Using [Spribg boot](https://github.com/vepo/wallet-hub/tree/spring-boot), I cannot run the jar file as required.

[No using Hibernate](https://github.com/vepo/wallet-hub/tree/without-hibernate), it was only 7% faster than using Hibernate. So it is preferred a solution using Hibernate with a build integrated unit test solution using JUnit and HSQLDB.

# Build

## Executable

To build the executable file, just execute:

```
./build.sh
```

This will compile and package the tool creating the `parser.jar` file.

## Infrastructure

To create the database infrastructure, just execute:

```
docker-compose up log-migrator
```

This command will create two docker containers:

* **log-db**: MySQL database container
* **log-db-migrator**: Container to create and update **log-db** schema. Using [flyway](https://flywaydb.org/). This container will update the schema and finish.

# Usage
To run the tool, just execute:

```
java -cp "parser.jar" com.ef.Parser --accesslog=access.log --startDate=2017-01-01.13:00:00 --duration=hourly --threshold=100 
```

The command is a little different from the specification. We just execute as an executable jar.

# Database

## Accessing

To access the database, use the following credentials

* **user**: log-user
* **password**: log-pw
* **database**: log-db

Or just execute a script:

```
./access-db.sh
```

## Schema

We store all request data into the database only normalizing agent to reduce the stored data.

```
CREATE TABLE agent (
	`id`         SERIAL,
	`description` TEXT NOT NULL,
	PRIMARY KEY(`id`)
);

CREATE TABLE log_access (
	`id`            SERIAL,
	`time`          DATETIME(3) NOT NULL,
	`ip`            VARCHAR(15) NOT NULL,
	`request`       VARCHAR(2048) NOT NULL,
	`response_code` INT NOT NULL,
	`agent_id`      BIGINT UNSIGNED,
	PRIMARY KEY(`id`),
	UNIQUE (`ip`, `time`),
	FOREIGN KEY (`agent_id`) REFERENCES `agent`(`id`) ON UPDATE CASCADE ON DELETE CASCADE
);

CREATE TABLE ip_blocked (
	`id` SERIAL,
	`ip` VARCHAR(15) NOT NULL,
	PRIMARY KEY(`id`),
	UNIQUE (`ip`),
	INDEX `idx_ip` (`ip`)
);
```

### Indexes

Besides the primary keys, I create indexes to ensure the data uniqueness.

In the table `log_access`, the log entry is unique if is generated by the same IP at the same time.

Int the table `ip_blocked`, there is only one entry by ip address.

## Quering

To find the IP that made more than a certain number of request (`100`) in a given time period (`2017-01-01.13:00:00 to 2017-01-01.14:00:00`), we should use the query bellow:

```
SELECT `ip` FROM log_access WHERE `time` >= '2017-01-01 13:00:00.000' AND `time` < '2017-01-01 14:00:00.000' GROUP BY `ip` HAVING COUNT(`ip`) > 100;
```

To list all request from a given IP (`192.168.228.188`) address:

```
SELECT * FROM log_access WHERE ip = '192.168.228.188';
```